# Token Usage Analysis

This document provides information about token usage in the NCERT Q&A Assistant application.

## What are Tokens?

Tokens are the basic units of text that language models process. They can be parts of words, full words, or punctuation. For example, the sentence "Hello, how are you?" might be tokenized as ["Hello", ",", "how", "are", "you", "?"]. The number of tokens affects:

1. **Processing time**: More tokens require more processing time
2. **API costs**: Most LLM providers charge based on token usage
3. **Context window limits**: LLMs have maximum token limits for input and output

## Token Usage Demo

We've created a special demo to analyze token usage across different prompting techniques. This helps us understand the efficiency and cost implications of each approach.

### Running the Demo

```bash
# Run the token usage comparison demo
python demo_token_usage.py
```

Or use the batch file:

```bash
demo_token_usage.bat
```

### What the Demo Shows

The demo tests different prompting techniques with the same questions and compares:

1. **Input tokens**: How many tokens are sent to the model
2. **Output tokens**: How many tokens are generated by the model
3. **Total tokens**: The sum of input and output tokens
4. **Response time**: How long the request takes to complete

### Token Usage by Technique

Different prompting techniques use varying amounts of tokens:

| Technique  | Input Tokens | Description                                 |
| ---------- | ------------ | ------------------------------------------- |
| Zero-Shot  | Lowest       | No examples included, just instructions     |
| One-Shot   | Moderate     | One example included in the prompt          |
| Multi-Shot | High         | Multiple examples included in the prompt    |
| Dynamic    | Moderate     | Question-specific template without examples |

### Understanding the Results

The demo outputs:

- A summary table with averages across all test cases
- Detailed tables for each test question
- A JSON file with complete results for further analysis

### Token Counting Method

Token counts are estimated using a simple whitespace and punctuation-based algorithm. While not exactly matching the tokenization of specific models like GPT or Gemini, it provides a reasonable approximation for comparison purposes.

## Optimizing Token Usage

Based on the demo results, consider these strategies:

1. **Use Zero-Shot** for simpler questions to minimize token usage
2. **Use One-Shot** for moderate complexity when consistency is needed
3. **Use Multi-Shot** for complex questions requiring precise formatting
4. **Use Dynamic** for specialized question types requiring specific structure

## Technical Implementation

Token usage information is:

1. Calculated in the backend using the `count_tokens()` function
2. Logged to the console with each API call
3. Returned in API responses as part of the response metadata
4. Displayed in the UI with each answer
